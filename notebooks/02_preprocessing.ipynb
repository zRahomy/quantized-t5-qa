{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89d0c985-d0c2-4d80-bdc4-92dc1d0b9ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = load_from_disk(\"../data/raw_squad\")\n",
    "\n",
    "model_checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7379739d-c492-4a20-a111-508972d99a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 10000\n",
      "Validation size: 2000\n"
     ]
    }
   ],
   "source": [
    "small_train = dataset[\"train\"].shuffle(seed=42).select(range(10000))\n",
    "small_val   = dataset[\"validation\"].shuffle(seed=42).select(range(2000))\n",
    "\n",
    "dataset = {\n",
    "    \"train\": small_train,\n",
    "    \"validation\": small_val\n",
    "}\n",
    "\n",
    "print(\"Train size:\", len(dataset[\"train\"]))\n",
    "print(\"Validation size:\", len(dataset[\"validation\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b1c431e-294b-4572-ae7f-05870f0dde8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 512\n",
    "max_target_length = 64\n",
    "\n",
    "def preprocess(example):\n",
    "    question = example[\"question\"]\n",
    "    context = example[\"context\"]\n",
    "    answer = example[\"answers\"][\"text\"][0]\n",
    "\n",
    "    input_text = f\"question: {question}  context: {context}\"\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        input_text,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        answer,\n",
    "        max_length=max_target_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad0bdd59-6c2a-473e-96e3-60c57f0b2f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6cd701eb06b496d94a6433ed492a7c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb26180c1494b939542de0754c1558e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized = {\n",
    "    \"train\": dataset[\"train\"].map(preprocess, remove_columns=dataset[\"train\"].column_names),\n",
    "    \"validation\": dataset[\"validation\"].map(preprocess, remove_columns=dataset[\"validation\"].column_names)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e44e31a3-8a19-4cd9-ad84-f14f776c4238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffca2d8d39649ca9c1b67d30f16402e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce167155e62f423cbfba4d91db363cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset saved.\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "tokenized = DatasetDict(tokenized)\n",
    "tokenized.save_to_disk(\"../data/tokenized_squad_small\")\n",
    "\n",
    "print(\"Tokenized dataset saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c531fd0f-0319-43ce-9649-d2fe74d7ae66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length: 512\n",
      "Target length: 64\n"
     ]
    }
   ],
   "source": [
    "sample = tokenized[\"train\"][0]\n",
    "print(\"Input length:\", len(sample[\"input_ids\"]))\n",
    "print(\"Target length:\", len(sample[\"labels\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
