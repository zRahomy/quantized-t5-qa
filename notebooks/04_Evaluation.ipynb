{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f384de0-b098-4444-8b5f-8e7fde55bac1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_from_disk\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74df22d3-40ef-4834-989b-ec7c39b329fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model & tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../model/final\" \n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model.eval()\n",
    "print(\"Model & tokenizer loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba86a7c5-c0c4-4fa0-8ad4-386c42f70cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n",
      "Dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "dataset_path = Path(\"../data/tokenized_squad_small\")\n",
    "dataset = load_from_disk(str(dataset_path))\n",
    "\n",
    "print(dataset)\n",
    "print(\"Dataset loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90bbdd38-9f78-4970-9fd6-4b9dbfd45089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(example):\n",
    "    input_ids = torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(example[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=64,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f760812d-18cc-473e-9933-162959449455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1852\n",
      "Reference : 1852\n"
     ]
    }
   ],
   "source": [
    "example = dataset[\"validation\"][0]\n",
    "\n",
    "prediction = predict_answer(example)\n",
    "\n",
    "labels = [t for t in example[\"labels\"] if t != -100]\n",
    "reference = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "print(\"Prediction:\", prediction)\n",
    "print(\"Reference :\", reference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f67b83bd-3a59-4f2b-b21d-dfe9176dcf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def normalize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def compute_exact(a, b):\n",
    "    return int(normalize(a) == normalize(b))\n",
    "\n",
    "def compute_f1(a, b):\n",
    "    a_tokens = normalize(a).split()\n",
    "    b_tokens = normalize(b).split()\n",
    "\n",
    "    common = Counter(a_tokens) & Counter(b_tokens)\n",
    "    num_same = sum(common.values())\n",
    "\n",
    "    if len(a_tokens) == 0 or len(b_tokens) == 0:\n",
    "        return int(a_tokens == b_tokens)\n",
    "\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "\n",
    "    precision = num_same / len(a_tokens)\n",
    "    recall = num_same / len(b_tokens)\n",
    "\n",
    "    return 2 * precision * recall / (precision + recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13c22397-aaf3-4bb9-b852-5035a1c1397c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [04:09<00:00,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match: 0.5745\n",
      "F1 Score   : 0.7570155827955984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "exact_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for example in tqdm(dataset[\"validation\"]):\n",
    "    pred = predict_answer(example)\n",
    "\n",
    "    labels = [t for t in example[\"labels\"] if t != -100]\n",
    "    ref = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    exact_scores.append(compute_exact(pred, ref))\n",
    "    f1_scores.append(compute_f1(pred, ref))\n",
    "\n",
    "exact_match = sum(exact_scores) / len(exact_scores)\n",
    "f1 = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "print(\"Exact Match:\", exact_match)\n",
    "print(\"F1 Score   :\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c59ef2b-2d60-4492-9018-0cb854e3edb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 ---\n",
      "Prediction: 1852\n",
      "Reference : 1852\n",
      "\n",
      "--- Example 2 ---\n",
      "Prediction: 1962\n",
      "Reference : 1962\n",
      "\n",
      "--- Example 3 ---\n",
      "Prediction: Horace Walpole\n",
      "Reference : Horace Walpole\n",
      "\n",
      "--- Example 4 ---\n",
      "Prediction: Shimer College\n",
      "Reference : several regional colleges and universities\n",
      "\n",
      "--- Example 5 ---\n",
      "Prediction: Jonathan Stewart\n",
      "Reference : Jonathan Stewart\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    example = dataset[\"validation\"][i]\n",
    "    pred = predict_answer(example)\n",
    "\n",
    "    labels = [t for t in example[\"labels\"] if t != -100]\n",
    "    ref = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(\"Prediction:\", pred)\n",
    "    print(\"Reference :\", ref)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
